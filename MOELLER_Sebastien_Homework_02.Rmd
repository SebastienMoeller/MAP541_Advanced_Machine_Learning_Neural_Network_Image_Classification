---
title: 'MAP 541 Homework 02'
author: 'Sebastien Moeller'
date: '14/03/2018'
output:
  html_document:
    fig_height: 4
    fig_width: 8
    number_sections: false
    toc: true
    toc_depth: 5
    toc_float: true
    theme: flatly
    highlight: haddock
# tan1
# darkunica
# deepskyblue3
# palevioletred
---

# Objectives
The purpose of this homework is to build an image recognition system, using transfer learning and fine-tuning based on pre tuned convolutional neural networks on ImageNet. (http://www.image-net.org/). I started with keras pre trained model (see for instance https://keras.rstudio.com/articles/applimonkeyions.html).

# Exercise 1 -- Image Net
#### What is Image Net?

#### ImageNet Cheese
How many different kinds of cheese can you find in ImageNet?

#### Best classifier
What is the best classifier on ImageNet and what is its error rate?

# Exercise 2 -- Build an image recognition system
Build an image recognition system for a 1000 everyday object monkeyegories (ImageNet ILSVRC) using Keras and TensorFlow.

#### Install keras
```{r}
# If the function 'is_keras_available()' returns FALSE install_keras(), if that doesn't work install package from github directly and run install_keras() afterwards:
# remove.packages('reticulate')
# remove.packages('tensorflow')
# remove.packages('keras')

# install.packages('devtools')
# require(devtools)
# install_github('rstudio/reticulate')
# install_github('rstudio/tensorflow')
# install_github('rstudio/keras')
if (!require('pacman')) install.packages('pacman')

pacman::p_load(tensorflow, reticulate, keras)

#install_keras()
#is_keras_available()
```

#### ResNet50
Define ResNet50 as you model and check its architecture
```{r}
model <- application_resnet50(weights <- 'imagenet')
summary(model)
```

#### Other pre-trained networks
What are the other pre trained networks available with keras? https://keras.rstudio.com/articles/applimonkeyions.html

The available networks currectly include:
Xception, VGG16, VGG19, ResNet50, InceptionV3, InceptionResNetV2, MobileNet, DenseNet, NASNet

#### Opening an Image
open an image ('my_image.jpg' in the following example) representing a single object (if possible represented in ImageNet)

```{r}
img_path <- 'hot_air_balloon.jpeg'
img <- image_load(img_path, target_size = c(224,224))
```

#### Reshape the image to fit the input format of your model
```{r}
x <- image_to_array(img)
x <- array_reshape(x, c(1, dim(x)))
```

#### Preprocess the input
```{r}
x <- imagenet_preprocess_input(x)
```

#### get the model predictions
```{r}
preds <- model %>% predict(x)
```

#### display the top 5 recognized objects. Do you find the one represented on your image?
```{r}
imagenet_decode_predictions(preds, top <- 5)[[1]]
```

# Exericse 3 -- Your Turn
### Binary Object Recognition
based on your previous work, build an binary object recognition (only two objects) by
transfer learning and fine tuning.

#### Choose Two Classes
I chose spider monkeys and labradors.

#### Download images
I downloaded 25 images for each class and setup my data with a training directory and a validation directory as follows:


#### Coding
proceed adapting the code from https://keras.rstudio.com/articles/applimonkeyions.html
```{r}
original_dataset_dir <- 'data/original'

base_dir <- 'data/cats_and_dogs'
dir.create(base_dir)

train_dir <- file.path(base_dir, 'train')
dir.create(train_dir)
validation_dir <- file.path(base_dir, 'validation')
dir.create(validation_dir)
test_dir <- file.path(base_dir, 'test')
dir.create(test_dir)

train_monkeys_dir <- file.path(train_dir, 'cats')
dir.create(train_monkeys_dir)

train_dogs_dir <- file.path(train_dir, 'dogs')
dir.create(train_dogs_dir)

validation_monkeys_dir <- file.path(validation_dir, 'cats')
dir.create(validation_monkeys_dir)

validation_dogs_dir <- file.path(validation_dir, 'dogs')
dir.create(validation_dogs_dir)

test_monkeys_dir <- file.path(test_dir, 'cats')
dir.create(test_monkeys_dir)

test_dogs_dir <- file.path(test_dir, 'dogs')
dir.create(test_dogs_dir)

fnames <- paste0('cat.', 1:15, '.jpg')
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(train_monkeys_dir)) 

fnames <- paste0('cat.', 16:19, '.jpg')
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(validation_monkeys_dir))

fnames <- paste0('cat.', 20:24, '.jpg')
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_monkeys_dir))

fnames <- paste0('dog.', 1:15, '.jpg')
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_dogs_dir))

fnames <- paste0('dog.', 16:19, '.jpg')
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_dogs_dir)) 

fnames <- paste0('dog.', 20:24, '.jpg')
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_dogs_dir))
```


Let's instantiate the VGG16 model.
```{r}
conv_base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)
```
You pass three arguments to the function:

-- weights specifies the weight checkpoint from which to initialize the model.

-- include_top refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the 1,000 classes from ImageNet. Because you intend to use your own densely connected classifier (with only two classes: monkey and dog), you don’t need to include it.

-- input_shape is the shape of the image tensors that you’ll feed to the network. This argument is purely optional: if you don’t pass it, the network will be able to process inputs of any size.

Here’s the detail of the architecture of the VGG16 convolutional base. It’s similar to the simple convnets you’re already familiar with:
```{r}
summary(conv_base)
```

At this point, there are two ways you could proceed:

-- Running the convolutional base over your dataset, recording its output to an array on disk, and then using this data as input to a standalone, densely connected classifier similar to those you saw in part 1 of this book. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique won’t allow you to use data augmentation.

-- Extending the model you have (conv_base) by adding dense layers on top, and running the whole thing end to end on the input data. This will allow you to use data augmentation, because every input image goes through the convolutional base every time it’s seen by the model. But for the same reason, this technique is far more expensive than the first.

We will proceed with the second option. Note that this technique is so expensive that you should only attempt it if you have access to a GPU – it’s absolutely intractable on a CPU.

Because models behave just like layers, you can add a model (like conv_base) to a sequential model just like you would add a layer.
```{r}
model <- keras_model_sequential() %>% 
  conv_base %>% 
  layer_flatten() %>% 
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

This is what the model looks like now:
```{r}
summary(model)
```
As you can see, the convolutional base of VGG16 has 14,714,688 parameters, which is very large. The classifier you’re adding on top has 2 million parameters.

Before you compile and train the model, it’s very important to freeze the convolutional base. Freezing a layer or set of layers means preventing their weights from being updated during training. If you don’t do this, then the representations that were previously learned by the convolutional base will be modified during training. Because the dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.

In Keras, you freeze a network using the freeze_weights() function:
```{r}
length(model$trainable_weights)
```

```{r}
freeze_weights(conv_base)
length(model$trainable_weights)
```
With this setup, only the weights from the two dense layers that you added will be trained. That’s a total of four weight tensors: two per layer (the main weight matrix and the bias vector). Note that in order for these changes to take effect, you must first compile the model. If you ever modify weight trainability after compilation, you should then recompile the model, or these changes will be ignored.

#### USING DATA AUGMENTATION
Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.

In Keras, this can be done by configuring a number of random transformations to be performed on the images read by an image_data_generator(). For example:
```{r}
train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)
```

Now we can train our model using the image data generator:
```{r}
# Note that the validation data shouldn't be augmented!
test_datagen <- image_data_generator(rescale = 1/255)  

train_generator <- flow_images_from_directory(
  train_dir,                  # Target directory  
  train_datagen,              # Data generator
  target_size = c(150, 150),  # Resizes all images to 150 × 150
  batch_size = 20,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)

validation_generator <- flow_images_from_directory(
  validation_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 10,
  epochs = 20,
  validation_data = validation_generator,
  validation_steps = 10
)
```







### Transfer Learning vs Fine Tuning
Is it better to do transfer learning and fine tuning or both?




